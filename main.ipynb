{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies and Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: asttokens in c:\\users\\loune\\documents\\programming\\hdi\\hdi\\lib\\site-packages (from -r requirements.txt (line 1)) (2.4.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\loune\\documents\\programming\\hdi\\hdi\\lib\\site-packages (from -r requirements.txt (line 2)) (0.4.6)\n",
      "Requirement already satisfied: comm in c:\\users\\loune\\documents\\programming\\hdi\\hdi\\lib\\site-packages (from -r requirements.txt (line 3)) (0.2.2)\n",
      "Requirement already satisfied: debugpy in c:\\users\\loune\\documents\\programming\\hdi\\hdi\\lib\\site-packages (from -r requirements.txt (line 4)) (1.8.2)\n",
      "Requirement already satisfied: decorator in c:\\users\\loune\\documents\\programming\\hdi\\hdi\\lib\\site-packages (from -r requirements.txt (line 5)) (5.1.1)\n",
      "Requirement already satisfied: executing in c:\\users\\loune\\documents\\programming\\hdi\\hdi\\lib\\site-packages (from -r requirements.txt (line 6)) (2.0.1)\n",
      "Collecting findspark (from -r requirements.txt (line 7))\n",
      "  Using cached findspark-2.0.1-py2.py3-none-any.whl.metadata (352 bytes)\n",
      "Requirement already satisfied: ipykernel in c:\\users\\loune\\documents\\programming\\hdi\\hdi\\lib\\site-packages (from -r requirements.txt (line 8)) (6.29.5)\n",
      "Requirement already satisfied: ipython in c:\\users\\loune\\documents\\programming\\hdi\\hdi\\lib\\site-packages (from -r requirements.txt (line 9)) (8.26.0)\n",
      "Requirement already satisfied: jedi in c:\\users\\loune\\documents\\programming\\hdi\\hdi\\lib\\site-packages (from -r requirements.txt (line 10)) (0.19.1)\n",
      "Requirement already satisfied: jupyter_client in c:\\users\\loune\\documents\\programming\\hdi\\hdi\\lib\\site-packages (from -r requirements.txt (line 11)) (8.6.2)\n",
      "Requirement already satisfied: jupyter_core in c:\\users\\loune\\documents\\programming\\hdi\\hdi\\lib\\site-packages (from -r requirements.txt (line 12)) (5.7.2)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\loune\\documents\\programming\\hdi\\hdi\\lib\\site-packages (from -r requirements.txt (line 13)) (0.1.7)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\loune\\documents\\programming\\hdi\\hdi\\lib\\site-packages (from -r requirements.txt (line 14)) (1.6.0)\n",
      "Collecting numpy (from -r requirements.txt (line 15))\n",
      "  Using cached numpy-2.0.0-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\loune\\documents\\programming\\hdi\\hdi\\lib\\site-packages (from -r requirements.txt (line 16)) (24.1)\n",
      "Collecting pandas (from -r requirements.txt (line 17))\n",
      "  Using cached pandas-2.2.2-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: parso in c:\\users\\loune\\documents\\programming\\hdi\\hdi\\lib\\site-packages (from -r requirements.txt (line 18)) (0.8.4)\n",
      "Requirement already satisfied: platformdirs in c:\\users\\loune\\documents\\programming\\hdi\\hdi\\lib\\site-packages (from -r requirements.txt (line 19)) (4.2.2)\n",
      "Requirement already satisfied: prompt_toolkit in c:\\users\\loune\\documents\\programming\\hdi\\hdi\\lib\\site-packages (from -r requirements.txt (line 20)) (3.0.47)\n",
      "Requirement already satisfied: psutil in c:\\users\\loune\\documents\\programming\\hdi\\hdi\\lib\\site-packages (from -r requirements.txt (line 21)) (6.0.0)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\loune\\documents\\programming\\hdi\\hdi\\lib\\site-packages (from -r requirements.txt (line 22)) (0.2.2)\n",
      "Collecting py4j (from -r requirements.txt (line 23))\n",
      "  Using cached py4j-0.10.9.7-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: Pygments in c:\\users\\loune\\documents\\programming\\hdi\\hdi\\lib\\site-packages (from -r requirements.txt (line 24)) (2.18.0)\n",
      "Collecting pyspark (from -r requirements.txt (line 25))\n",
      "  Using cached pyspark-3.5.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\loune\\documents\\programming\\hdi\\hdi\\lib\\site-packages (from -r requirements.txt (line 26)) (2.9.0.post0)\n",
      "Collecting pytz (from -r requirements.txt (line 27))\n",
      "  Using cached pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: pywin32 in c:\\users\\loune\\documents\\programming\\hdi\\hdi\\lib\\site-packages (from -r requirements.txt (line 28)) (306)\n",
      "Requirement already satisfied: pyzmq in c:\\users\\loune\\documents\\programming\\hdi\\hdi\\lib\\site-packages (from -r requirements.txt (line 29)) (26.0.3)\n",
      "Collecting setuptools (from -r requirements.txt (line 30))\n",
      "  Using cached setuptools-70.3.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: six in c:\\users\\loune\\documents\\programming\\hdi\\hdi\\lib\\site-packages (from -r requirements.txt (line 31)) (1.16.0)\n",
      "Requirement already satisfied: stack-data in c:\\users\\loune\\documents\\programming\\hdi\\hdi\\lib\\site-packages (from -r requirements.txt (line 32)) (0.6.3)\n",
      "Collecting tabulate (from -r requirements.txt (line 33))\n",
      "  Using cached tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Requirement already satisfied: tornado in c:\\users\\loune\\documents\\programming\\hdi\\hdi\\lib\\site-packages (from -r requirements.txt (line 34)) (6.4.1)\n",
      "Requirement already satisfied: traitlets in c:\\users\\loune\\documents\\programming\\hdi\\hdi\\lib\\site-packages (from -r requirements.txt (line 35)) (5.14.3)\n",
      "Collecting tzdata (from -r requirements.txt (line 36))\n",
      "  Using cached tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\loune\\documents\\programming\\hdi\\hdi\\lib\\site-packages (from -r requirements.txt (line 37)) (0.2.13)\n",
      "Using cached findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
      "Using cached numpy-2.0.0-cp312-cp312-win_amd64.whl (16.2 MB)\n",
      "Using cached pandas-2.2.2-cp312-cp312-win_amd64.whl (11.5 MB)\n",
      "Using cached py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "Using cached pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "Using cached setuptools-70.3.0-py3-none-any.whl (931 kB)\n",
      "Using cached tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Using cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Installing collected packages: pytz, py4j, findspark, tzdata, tabulate, setuptools, pyspark, numpy, pandas\n",
      "Successfully installed findspark-2.0.1 numpy-2.0.0 pandas-2.2.2 py4j-0.10.9.7 pyspark-3.5.1 pytz-2024.1 setuptools-70.3.0 tabulate-0.9.0 tzdata-2024.1\n"
     ]
    }
   ],
   "source": [
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3 as sql\n",
    "import os\n",
    "import hashlib\n",
    "import setuptools  # for distutils\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple, List, Callable, Any, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Loune\\\\spark-3.5.1-bin-hadoop3'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import col, md5, monotonically_increasing_id\n",
    "from pyspark.sql.types import StringType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKING_DIRECTORY = \"data/\"\n",
    "OUTPUTS_DIRECTORY = \"outputs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(directory: str) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Reads all CSV files in the specified directory and stores their contents in a dictionary.\n",
    "\n",
    "    Parameters:\n",
    "    directory (str): The path to the directory containing CSV files.\n",
    "\n",
    "    Returns:\n",
    "    Dict[str, pd.DataFrame]: A dictionary where keys are CSV file names and values are their contents as DataFrames.\n",
    "    \"\"\"\n",
    "\n",
    "    csv_files = [f for f in os.listdir(directory) if f.endswith(\".csv\")]\n",
    "    csv_files_content = {\n",
    "        file_name: pd.read_csv(os.path.join(directory, file_name))\n",
    "        for file_name in csv_files\n",
    "    }\n",
    "\n",
    "    return csv_files_content\n",
    "\n",
    "\n",
    "data = read_files(WORKING_DIRECTORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_name</th>\n",
       "      <th>num_variables</th>\n",
       "      <th>num_categorical_variables</th>\n",
       "      <th>num_rows</th>\n",
       "      <th>num_missing_values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>er_prs_f.csv</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ir_act_v.csv</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ir_ben_r.csv</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ir_spe_v.csv</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t_mcoaae.csv</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      data_name  num_variables  num_categorical_variables  num_rows  \\\n",
       "0  er_prs_f.csv              8                          3        20   \n",
       "1  ir_act_v.csv              2                          1         2   \n",
       "2  ir_ben_r.csv              6                          1        20   \n",
       "3  ir_spe_v.csv              2                          1         2   \n",
       "4  t_mcoaae.csv              2                          1         2   \n",
       "\n",
       "   num_missing_values  \n",
       "0                  21  \n",
       "1                   0  \n",
       "2                   0  \n",
       "3                   0  \n",
       "4                   0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata = {\n",
    "    \"data_name\": [],\n",
    "    \"num_variables\": [],\n",
    "    \"num_categorical_variables\": [],\n",
    "    \"num_rows\": [],\n",
    "    \"num_missing_values\": [],\n",
    "}\n",
    "\n",
    "for file_name, df in data.items():\n",
    "    metadata[\"data_name\"].append(file_name)\n",
    "    metadata[\"num_variables\"].append(df.shape[1])\n",
    "    metadata[\"num_categorical_variables\"].append(\n",
    "        df.select_dtypes(include=[\"object\", \"category\"]).shape[1]\n",
    "    )\n",
    "    metadata[\"num_rows\"].append(df.shape[0])\n",
    "    metadata[\"num_missing_values\"].append(df.isnull().sum().sum())\n",
    "\n",
    "metadata_df = pd.DataFrame(metadata)\n",
    "metadata_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, df in data.items():\n",
    "    print(f\"File name: {name}\")\n",
    "    print(df.head(3).to_markdown(index=False))\n",
    "    print(\"*****\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define a hash function that will be used to hash the data\n",
    "def md5_function(input_str: str) -> int:\n",
    "    return int(hashlib.md5(input_str.encode()).hexdigest(), 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def md5_for_database(input_str: str) -> int:\n",
    "    return int(hashlib.md5(input_str.encode()).hexdigest(), 16) % (\n",
    "        10**15\n",
    "    )  # to fit in db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Person Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Person Table Construction\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Column name in <br> OMOP Table (Person)</th>\n",
    "    <th>Type</th>\n",
    "    <th>Corresponding column <br> in SNDS </th>\n",
    "    <th>Comments or transformations</th>\n",
    "    <th>Source</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>person_id</td>\n",
    "    <td>integer</td>\n",
    "    <td>num_enq (IR_BEN_R)</td>\n",
    "    <td>Turn into integer <br> using hash function.</td>\n",
    "    <td><a href=\"https://ohdsi.github.io/CommonDataModel/cdm53.html#person\">Ref 1</a></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>gender_concept_id</td>\n",
    "    <td>integer</td>\n",
    "    <td>ben_sex_cod (IR_BEN_R)</td>\n",
    "    <td>1 <- 8507 & 2 <- 8532</td>\n",
    "    <td><a href=\"https://ohdsi.github.io/CommonDataModel/cdm53.html#person\">Ref 1</a> & <a href=\"https://athena.ohdsi.org/search-terms/terms?domain=Gender&standardConcept=Standard&page=1&pageSize=15&query=\">Ref é</a> </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>year_of_birth</td>\n",
    "    <td>integer</td>\n",
    "    <td>ben_nai_ann (IR_BEN_R)</td>\n",
    "    <td>No transfo needed <br> If no year_of_birth, drop the row.</td>\n",
    "    <td><a href=\"https://ohdsi.github.io/CommonDataModel/cdm53.html#person\">Ref 1</a></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>month_of_birth</td>\n",
    "    <td>integer</td>\n",
    "    <td>ben_nai_moi (IR_BEN_R)</td>\n",
    "    <td>No transfo needed</td>\n",
    "    <td><a href=\"https://ohdsi.github.io/CommonDataModel/cdm53.html#person\">Ref 1</a></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>person_source_value</td>\n",
    "    <td>string</td>\n",
    "    <td>num_enq (IR_BEN_R)</td>\n",
    "    <td>No transfo needed</td>\n",
    "    <td><a href=\"https://ohdsi.github.io/CommonDataModel/cdm53.html#person\">Ref 1</a></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>gender_source_value</td>\n",
    "    <td>string</td>\n",
    "    <td>ben_sex_cod (IR_BEN_R)</td>\n",
    "    <td>Turn into athena <br> string code for gender. <br> 1 <- 'M', 2 <- 'F'</td>\n",
    "    <td><a href=\"https://ohdsi.github.io/CommonDataModel/cdm53.html#person\">Ref 1</a> & <a href=\"https://athena.ohdsi.org/search-terms/terms?domain=Gender&standardConcept=Standard&page=1&pageSize=15&query=\">Ref 2</a></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>location_id</td>\n",
    "    <td>integer</td>\n",
    "    <td>ben_res_dpt & ben_res_reg <br> (IR_BEN_R)</td>\n",
    "    <td>Compute concat(ben_res_dpt, <br> ben_res_reg) </td>\n",
    "    <td>Unique key given to a unique Location. <a href=\"https://ohdsi.github.io/CommonDataModel/cdm53.html#location\">Ref 3\"</a></td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. All the info we need in order to construct the new table already are in IR_BEN_R, at first sight we don't need to perfom any table join.\n",
    "1. We start by defining the functions used to apply the transformations on the data.\n",
    "2. Then we create each transformation function using the transformation rules defined in the table above.\n",
    "3. Finally, we call the function that applies the transformations on the data to obtain the new transformed dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_apply(func: Callable[..., Any], args: List[Any]) -> Any:\n",
    "    \"\"\"Helper function to apply a function with arguments and catch exceptions.\n",
    "\n",
    "    Parameters\n",
    "    - func (Callable[..., Any]): A function to apply.\n",
    "    - args (List[Any]): A list of arguments to pass to the function.\n",
    "\n",
    "    Returns\n",
    "    - Any: The result of applying the function with the provided arguments, or None if an exception occurred.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return func(*args)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing with function {func.__name__}: {e}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_transformations(\n",
    "    transformations: Dict[str, Tuple[Callable[..., Any], List[Any]]]\n",
    ") -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Generates a new DataFrame based on transformations.\n",
    "\n",
    "    Parameters:\n",
    "    - transformations (Dict[str, Tuple[Callable[..., Any], List[Any]]]): A dictionary where keys are column names of the new DataFrame.\n",
    "      The values are tuples containing a function and a list of arguments. Each column is generated by executing the function with its arguments.\n",
    "\n",
    "    Returns:\n",
    "    - Optional[pd.DataFrame]: A new DataFrame generated based on the provided transformations, or None if transformations didn't give any result.\n",
    "    \"\"\"\n",
    "    new_data = {}\n",
    "\n",
    "    for col_name, (func, args) in transformations.items():\n",
    "        try:\n",
    "            new_data[col_name] = safe_apply(func, args)\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "\n",
    "    if new_data:\n",
    "        return pd.DataFrame(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create the transformation functions that we'll use to apply the rules\n",
    "\n",
    "# Some columns are generated without applying any transformation.\n",
    "identity: Callable[[pd.Series], pd.Series] = lambda x: x\n",
    "\n",
    "# Hashed value\n",
    "hashed_value: Callable[[pd.Series], pd.Series] = lambda x: x.apply(md5_function)\n",
    "\n",
    "\n",
    "# Column \"gender_concept_id\" is generated by assigning new values for each gender\n",
    "def apply_mapping_rules(\n",
    "    original_col: pd.Series, mapping_rules: Dict[Any, Any]\n",
    ") -> pd.Series:\n",
    "    return original_col.apply(\n",
    "        lambda x: mapping_rules[x] if x in mapping_rules else None\n",
    "    )\n",
    "\n",
    "\n",
    "# Column location_id is created by concatenating integer columns.\n",
    "def concatenate_codes(codes1: pd.Series, codes2: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Concatenate two series containing int codes and return the result as a new series,\n",
    "    with each element formed by concatenating corresponding elements from the input series.\n",
    "\n",
    "    Args:\n",
    "    codes1 (pd.Series): First series of codes.\n",
    "    codes2 (pd.Series): Second series of codes.\n",
    "\n",
    "    Returns:\n",
    "    pd.Series: The concatenated result as a series.\n",
    "\n",
    "    Raises:\n",
    "    ValueError: If the input series do not have the same length.\n",
    "    \"\"\"\n",
    "    if len(codes1) != len(codes2):\n",
    "        raise ValueError(\"Input series must have the same length\")\n",
    "\n",
    "    concatenated = codes1.astype(str) + codes2.astype(str)\n",
    "\n",
    "    return concatenated.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define the transformation rules to create and the data\n",
    "\n",
    "TABLE_NAME = \"ir_ben_r.csv\"\n",
    "\n",
    "transformations = {\n",
    "    \"person_id\": (hashed_value, [data[TABLE_NAME][\"NUM_ENQ\"]]),\n",
    "    \"gender_concept_id\": (\n",
    "        apply_mapping_rules,\n",
    "        [data[TABLE_NAME][\"ben_sex_cod\"], {1: 8507, 2: 8532}],\n",
    "    ),\n",
    "    \"year_of_birth\": (identity, [data[TABLE_NAME][\"ben_nai_ann\"]]),\n",
    "    \"month_of_birth\": (identity, [data[TABLE_NAME][\"ben_nai_moi\"]]),\n",
    "    \"person_source_value\": (identity, [data[TABLE_NAME][\"NUM_ENQ\"]]),\n",
    "    \"location_id\": (\n",
    "        concatenate_codes,\n",
    "        [data[TABLE_NAME][\"ben_res_dpt\"], data[TABLE_NAME][\"ben_res_reg\"]],\n",
    "    ),\n",
    "    \"gender_source_value\": (\n",
    "        apply_mapping_rules,\n",
    "        [data[TABLE_NAME][\"ben_sex_cod\"], {1: \"M\", 2: \"F\"}],\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We apply all the rules to obtain the new Person Table\n",
    "person_table = apply_transformations(transformations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_table.to_csv(os.path.join(OUTPUTS_DIRECTORY, \"person_table.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Care Site Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Care Site Table Construction\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Column name in <br> OMOP Table (Care Site)</th>\n",
    "    <th>Type</th>\n",
    "    <th>Corresponding column <br> & table in SNDS </th>\n",
    "    <th>Comments or transformations</th>\n",
    "    <th>Source</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>care_site_id</td>\n",
    "    <td>integer</td>\n",
    "    <td>eta_num & soc_rai <br> (t_mcoaae) </td>\n",
    "    <td>We derive it from (str(eta_num), soc_rai) <br> by applying a hash function.</td>\n",
    "    <td><a href=\"https://ohdsi.github.io/CommonDataModel/cdm53.html#care_site\">Ref 4</a></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>care_site_name</td>\n",
    "    <td>string</td>\n",
    "    <td>soc_rai (t_mcoaae)</td>\n",
    "    <td>No transformation needed.</td>\n",
    "    <td><a href=\"https://ohdsi.github.io/CommonDataModel/cdm53.html#care_site\">Ref 4</a></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>location_id</td>\n",
    "    <td>integer</td>\n",
    "    <td>location_id (LOCATION) <br> eta_num (t_mcoaae)</td>\n",
    "    <td>No transformation needed.</td>\n",
    "    <td>eta_num is the finess number, it is unique and it holds <br> a geographical meaning, we can use it as location_id</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>care_site_source_value</td>\n",
    "    <td>string</td>\n",
    "    <td>soc_rai (t_mcoaae)</td>\n",
    "    <td>In this case, we use care_site_name <br> because we don't have other info.</td>\n",
    "    <td><a href=\"https://ohdsi.github.io/CommonDataModel/cdm53.html#care_site\">Ref 4</a></td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_query = \"\"\"DROP TABLE IF EXISTS care_site\"\"\"\n",
    "\n",
    "create_query = \"\"\"\n",
    "CREATE TABLE care_site (\n",
    "    care_site_id INTEGER PRIMARY KEY,\n",
    "    care_site_name TEXT,\n",
    "    location_id INTEGER,\n",
    "    care_site_source_value TEXT\n",
    ")\n",
    "\"\"\"\n",
    "def populate_table_query(table_name: str) -> str:\n",
    "    \"\"\"Returns the query to populate the care_site table.\n",
    "\n",
    "    Parameters:\n",
    "    - table_name (str): The name of the table containing the data.\n",
    "\n",
    "    Returns:\n",
    "    - str: The query to populate the care_site table.\n",
    "    \"\"\"\n",
    "\n",
    "    return f\"\"\"INSERT INTO care_site (care_site_id, care_site_name, location_id, care_site_source_value)\n",
    "    SELECT \n",
    "        md5(cast({table_name}.eta_num as text) || {table_name}.soc_rai), \n",
    "        {table_name}.soc_rai AS care_site_name,\n",
    "        {table_name}.eta_num AS location_id, \n",
    "        {table_name}.soc_rai AS care_site_source_value\n",
    "    FROM {table_name}\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sql.connect(\"outputs/db.sql\")\n",
    "try:\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # we create a hash function\n",
    "    conn.create_function(\"md5\", 1, md5_for_database)\n",
    "\n",
    "    # we drop the table care_site if it exists and create it\n",
    "    cursor.execute(drop_query)\n",
    "    cursor.execute(create_query)\n",
    "\n",
    "    # We write the tables finess and t_mcoaae into the database to use them\n",
    "    tab_name = \"t_mcoaae\"\n",
    "    data[\"t_mcoaae.csv\"].to_sql(tab_name, conn, index=False, if_exists=\"replace\")\n",
    "\n",
    "    # We populate the care_site table\n",
    "    cursor.execute(populate_table_query(tab_name))\n",
    "    conn.commit()\n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")\n",
    "    traceback.print_exc()\n",
    "    conn.rollback()  # Rollback changes if an error occurs\n",
    "\n",
    "finally:\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "care_site = None\n",
    "conn = sql.connect(f\"{OUTPUTS_DIRECTORY}db.sql\")\n",
    "\n",
    "try:\n",
    "    care_site = pd.read_sql_query(\"SELECT * FROM care_site\", conn)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")\n",
    "finally:\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "care_site"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Provider Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provider Table Construction\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Column name in <br> OMOP Table (Provider)</th>\n",
    "    <th>Type</th>\n",
    "    <th>Corresponding column <br> & table in SNDS </th>\n",
    "    <th>Comments or transformations</th>\n",
    "    <th>Source</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>provider_id</td>\n",
    "    <td>integer</td>\n",
    "    <td>None</td>\n",
    "    <td>autogenerated number.</td>\n",
    "    <td><a href=\"https://ohdsi.github.io/CommonDataModel/cdm53.html#provider\">Ref 5</a></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>specialty_source_value</td>\n",
    "    <td>string</td>\n",
    "    <td>Label <br> (ir_act_v.csv & ir_spe_v.csv)</td>\n",
    "    <td>No transformation.</td>\n",
    "    <td><a href=\"https://ohdsi.github.io/CommonDataModel/cdm53.html#provider\">Ref 5</a></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>specialty_concept_id</td>\n",
    "    <td>integer</td>\n",
    "    <td>pfs_spe_cod (ir_spe_v) & <br> pfs_act_nat (ir_act_v)</td>\n",
    "    <td>map each speciality code <br> with the accepted concepts in OMOP.</td>\n",
    "    <td><a href=\"https://ohdsi.github.io/CommonDataModel/cdm53.html#provider\">Ref 5</a> & <a href=\"https://athena.ohdsi.org/search-terms/terms?domain=Provider&standardConcept=Standard&page=1&pageSize=15&query=\">Ref 6</a></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>provider_source_value</td>\n",
    "    <td>string</td>\n",
    "    <td>pfs_spe_cod (ir_spe_v) & <br> pfs_act_nat (ir_act_v)</td>\n",
    "    <td>No transformation needed, except str() casting.</td>\n",
    "    <td><a href=\"https://ohdsi.github.io/CommonDataModel/cdm53.html#provider\">Ref 5</a></td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Spark\n",
    "configuration = (\n",
    "    SparkConf().setAppName(\"Health Data Interoperability\").setMaster(\"local[2]\")\n",
    ")\n",
    "\n",
    "sc = SparkContext.getOrCreate(conf=configuration)\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.config(conf=configuration).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Pandas DataFrame to Spark DataFrame\n",
    "#act = spark.createDataFrame(data[\"ir_act_v.csv\"])\n",
    "#spe = spark.createDataFrame(data[\"ir_spe_v.csv\"])\n",
    "\n",
    "# Or Read from files:\n",
    "act = spark.read.csv(f\"{WORKING_DIRECTORY}/ir_act_v.csv\", header=True, sep=\",\")\n",
    "spe = spark.read.csv(f\"{WORKING_DIRECTORY}/ir_spe_v.csv\", header=True, sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kinésithérapeute - Pharmacien - Médecin généraliste - Radiologue\n",
    "speciality_concept_mapping = {50: 38003810, 26: 38004089, 1: 38004446, 6: 45756825}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by joigning the two tables\n",
    "combined_df = act.select(col(\"pfs_act_nat\").alias(\"code\"), col(\"label\")).union(\n",
    "    spe.select(col(\"pfs_spe_cod\"), col(\"label\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------+\n",
      "|code|              label|\n",
      "+----+-------------------+\n",
      "|  26|   Kinésithérapeute|\n",
      "|  50|         Pharmacien|\n",
      "|   1|Médecin généraliste|\n",
      "|   6|         Radiologue|\n",
      "+----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "combined_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then, we apply the necessary transformations\n",
    "new_df = (\n",
    "    combined_df.withColumn(\"provider_id\", monotonically_increasing_id() + 1)\n",
    "    .withColumn(\"speciality_source_value\", col(\"label\"))\n",
    "    .withColumn(\"specialty_concept_id\", col(\"code\").cast(\"int\"))\n",
    "    .replace(speciality_concept_mapping, subset=[\"specialty_concept_id\"])\n",
    "    .withColumn(\"provider_source_value\", col(\"code\").cast(StringType()))\n",
    "    .select(\n",
    "        \"provider_id\",\n",
    "        \"speciality_source_value\",\n",
    "        \"specialty_concept_id\",\n",
    "        \"provider_source_value\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------------+--------------------+---------------------+\n",
      "|provider_id|speciality_source_value|specialty_concept_id|provider_source_value|\n",
      "+-----------+-----------------------+--------------------+---------------------+\n",
      "|          1|       Kinésithérapeute|            38004089|                   26|\n",
      "|          2|             Pharmacien|            38003810|                   50|\n",
      "| 8589934593|    Médecin généraliste|            38004446|                    1|\n",
      "| 8589934594|             Radiologue|            45756825|                    6|\n",
      "+-----------+-----------------------+--------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.write.csv(f\"{OUTPUTS_DIRECTORY}provider.csv\", header=True, mode=\"overwrite\")\n",
    "\n",
    "# OR: write with pandas if errors with hadoop path\n",
    "#pandas_df = new_df.toPandas()\n",
    "#pandas_df.to_csv(f\"{OUTPUTS_DIRECTORY}provider.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hdi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
